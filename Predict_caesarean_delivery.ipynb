{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.impute import SimpleImputer, MissingIndicator\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression, Ridge, RidgeCV, LassoCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, precision_recall_curve,precision_score, recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 80 entries, 0 to 79\n",
      "Data columns (total 7 columns):\n",
      "Unnamed: 0        80 non-null int64\n",
      "age               80 non-null object\n",
      "delivery_no       80 non-null object\n",
      "delivery_time     80 non-null int64\n",
      "blood_pressure    80 non-null int64\n",
      "heart_problem     80 non-null int64\n",
      "caesarian         80 non-null int64\n",
      "dtypes: int64(5), object(2)\n",
      "memory usage: 4.5+ KB\n"
     ]
    }
   ],
   "source": [
    "#reading the data\n",
    "\n",
    "dataset = pd.read_csv('caesarian.csv')\n",
    "#print(dataset)\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputating of missing values\n",
    "\n",
    "dataset =dataset.replace('-','NaN')\n",
    "\n",
    "#Replacing the missing age values with the integer equal to the round down of the mean of age\n",
    "\n",
    "age=dataset['age']\n",
    "imp_mean =SimpleImputer(missing_values=np.NaN, strategy='mean' )\n",
    "fit_age=imp_mean.fit(age.values.reshape(-1, 1))\n",
    "imp_age=imp_mean.transform(age.values.reshape(-1, 1))\n",
    "imp_age=np.floor(imp_age)\n",
    "#print(imp_age.shape)\n",
    "#print(imp_age)\n",
    "\n",
    "\n",
    "#Replacing the missing delivery number values with the most frequent delivery number\n",
    "\n",
    "delivery_no =dataset['delivery_no']\n",
    "imp_most_freq =SimpleImputer(missing_values='NaN',strategy='most_frequent')\n",
    "fit_delivery_no=imp_most_freq.fit(delivery_no.values.reshape(-1, 1))\n",
    "imp_delivery_no=imp_most_freq.transform(delivery_no.values.reshape(-1, 1))\n",
    "\n",
    "#print(imp_delivery_no.shape)\n",
    "#print(imp_delivery_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the categorical variables using the one-hot encoding\n",
    "\n",
    "\n",
    "inp1=pd.DataFrame(imp_age)\n",
    "inp2=pd.DataFrame(imp_delivery_no)\n",
    "#inp2=pd.get_dummies(pd.DataFrame(imp_delivery_no))\n",
    "inp3=pd.get_dummies(dataset['delivery_time'])\n",
    "inp4=pd.get_dummies(dataset['blood_pressure'])\n",
    "inp5=dataset['heart_problem']\n",
    "\n",
    "frames1=[inp1,inp2,inp3,inp4,inp5]\n",
    "inp=pd.concat(frames1,axis=1)\n",
    "out = dataset['caesarian']\n",
    "#print(inp.shape)\n",
    "#print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pantp\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype uint8, int64, float64, object were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\pantp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DataConversionWarning: Data with input dtype uint8, int64, float64, object were all converted to float64 by StandardScaler.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# scaling the  data\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(inp)\n",
    "scaled = scaler.transform(inp)\n",
    "print(scaled.shape)\n",
    "#print(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/alpha =  [0.06734151]\n",
      "coef =  [[ 0.0330783   0.06878643  0.12774803 -0.05571059 -0.09866472  0.13769898\n",
      "  -0.17581956  0.06531996  0.22321539]]\n",
      "accuracy =  0.7125\n",
      "[1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0\n",
      " 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1\n",
      " 1 0 1 1 1 0]\n",
      "[0.67857143 0.69230769 0.57692308]\n",
      "Average predicted accuracy = 0.6492673992673993\n"
     ]
    }
   ],
   "source": [
    "# train logistic regression & cross validation using accuracy scoring\n",
    "#default scoring option used is accuracy\n",
    "\n",
    "clf_L2 = LogisticRegressionCV(Cs=100, cv=3, penalty='l2', random_state=0, multi_class='multinomial',\n",
    "                              solver='lbfgs', n_jobs=-1,max_iter=1000)\n",
    "clf_L2.fit (scaled, out)\n",
    "\n",
    "print('1/alpha = ', clf_L2.C_) \n",
    "print('coef = ', clf_L2.coef_)\n",
    "print('accuracy = ', clf_L2.score(scaled,out))\n",
    "\n",
    "\n",
    "#Predicting the output data:\n",
    "\n",
    "pred = clf_L2.predict(scaled)\n",
    "print(pred)\n",
    "#print('accuracy = ', clf_L2.score(scaled,pred))\n",
    "\n",
    "#Average predicted accuracy\n",
    "\n",
    "avg=cross_val_score(clf_L2, scaled, out, groups=None, scoring='accuracy',cv=3)\n",
    "print(avg)\n",
    "print('Average predicted accuracy =',avg.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.575     , 0.73469388, 1.        ]), array([1.       , 0.7826087, 0.       ]), array([0, 1], dtype=int64))\n",
      "0.7346938775510204\n",
      "0.782608695652174\n",
      "[1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0\n",
      " 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1\n",
      " 1 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "#check the precision, recall and accuracy\n",
    "\n",
    "#print(accuracy_score(out,pred))\n",
    "print(precision_recall_curve(out,pred))\n",
    "print(precision_score(out,pred))\n",
    "print(recall_score(out,pred))\n",
    "print(pred)\n",
    "p_pred=clf_L2.predict_proba(scaled)\n",
    "#print(p_pred)\n",
    "\n",
    "\n",
    "#probabilities:\n",
    "\n",
    "#print(p_tp)\n",
    "#print(p_tn)\n",
    "#print(p_fp)\n",
    "#print(p_fn)\n",
    "\n",
    "#confusion matrix and expected value\n",
    "\n",
    "#C = confusion_matrix(out,pred)\n",
    "#print('Confusion matrix = ', C)\n",
    "#p_tn, p_fp, p_fn, p_tp = C.ravel()/np.sum(C)\n",
    "#EV = 200*p_tp-400*p_tn-450*p_fn +0*p_fp\n",
    "#print('Expected_value', EV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected_value -71.25\n",
      "Confusion matrix =  [[21 13]\n",
      " [10 36]]\n"
     ]
    }
   ],
   "source": [
    "#create a function called underlying_EV_score(out,p_pred) that calculates the EV of an estimator\n",
    "\n",
    "def underlying_EV_score(out,p_pred,sample_weight=None):\n",
    "    pred = np.array([p_pred >= 0.66], dtype=np.int32)[0]\n",
    "    C = confusion_matrix(out,pred)\n",
    "    print('Confusion matrix = ', C)\n",
    "    p_tn, p_fp, p_fn, p_tp = C.ravel()/np.sum(C)\n",
    "    EV = 200*p_tp-400*p_tn-450*p_fn +0*p_fp\n",
    "    print('Expected_value', EV)\n",
    "    return EV\n",
    "\n",
    "print('expected_value',EV)\n",
    "print('Confusion matrix = ', C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_scorer(underlying_EV_score, needs_proba=True)\n",
      "LogisticRegressionCV(Cs=100, class_weight=None, cv=3, dual=False,\n",
      "           fit_intercept=True, intercept_scaling=1.0, max_iter=1000,\n",
      "           multi_class='multinomial', n_jobs=-1, penalty='l2',\n",
      "           random_state=0, refit=True,\n",
      "           scoring=make_scorer(underlying_EV_score, needs_proba=True),\n",
      "           solver='lbfgs', tol=0.0001, verbose=0)\n",
      "[1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0\n",
      " 0 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1\n",
      " 1 0 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "#making a scorer from performance matrix\n",
    "\n",
    "myEVscorer = make_scorer(underlying_EV_score, greater_is_better=True, needs_proba=True, needs_threshold=False)\n",
    "print(myEVscorer)\n",
    "\n",
    "# train logistic regression & cross validation using expected value as scoring function\n",
    "\n",
    "ev_clf_L2 = LogisticRegressionCV(Cs=100,cv=3,penalty='l2',random_state=0,multi_class='multinomial',solver='lbfgs',\n",
    "                                 max_iter=1000, n_jobs=-1, scoring=myEVscorer)\n",
    "ev_clf_L2.fit(scaled,out)\n",
    "print(ev_clf_L2)\n",
    "\n",
    "#predicting the data\n",
    "\n",
    "pred_2=ev_clf_L2.predict(scaled)\n",
    "print(pred_2)\n",
    "p_preda=ev_clf_L2.predict_proba(scaled)\n",
    "#print(p_preda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/alpha =  [0.20565123]\n",
      "coef =  [[ 0.03456609  0.08717588  0.16484285 -0.06559926 -0.13360276  0.17618054\n",
      "  -0.21966848  0.07747077  0.28217377]]\n",
      "Accuracy 0.725\n",
      "Confusion matrix =  [[24 10]\n",
      " [12 34]]\n",
      "EV Value for this model -102.5\n",
      "Mean EV =  [-194.18498168]\n",
      "SE EV =  [19.89544824]\n"
     ]
    }
   ],
   "source": [
    "#coefficient values\n",
    "\n",
    "print('1/alpha = ', ev_clf_L2.C_) \n",
    "print('coef = ', ev_clf_L2.coef_)\n",
    "\n",
    "#accuracy of the model\n",
    "print('Accuracy',accuracy_score(out,pred_2))\n",
    "\n",
    "#expecte value on whole training data:\n",
    "C_new = confusion_matrix(out,pred_2)\n",
    "print('Confusion matrix = ', C_new)\n",
    "p_tn, p_fp, p_fn, p_tp = C_new.ravel()/np.sum(C_new)\n",
    "EV_new = 200*p_tp-400*p_tn-450*p_fn +0*p_fp\n",
    "print('EV Value for this model',EV_new)\n",
    "\n",
    "\n",
    "# print('EV  scores = ', ev_clf_L2.scores_[1].mean(axis=0))\n",
    "# print('C inv.reg  = ', ev_clf_L2.Cs_)\n",
    "\n",
    "mean_EV = ev_clf_L2.scores_[1].mean(axis=0)\n",
    "se_EV = ev_clf_L2.scores_[1].std(axis=0)/np.sqrt(ev_clf_L2.cv)\n",
    "print('Mean EV = ', mean_EV[ev_clf_L2.Cs_ == ev_clf_L2.C_])\n",
    "print('SE EV = ', se_EV[ev_clf_L2.Cs_ == ev_clf_L2.C_])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  delivery_no  delivery_time  blood_pressure  heart_problem\n",
      "0   35            1              2               0              0\n"
     ]
    }
   ],
   "source": [
    "#predicting the final question_9:\n",
    "\n",
    "#reading the data\n",
    "dataset_final = pd.read_csv('caesarian_final_pred.csv')\n",
    "#print(dataset_final)\n",
    "\n",
    "#Encoding the categorical variables using the one-hot encoding\n",
    "\n",
    "inp1_final=dataset_final['age']\n",
    "inp2_final=dataset_final['delivery_no']\n",
    "#inp2_final=pd.get_dummies(pd.DataFrame(imp_delivery_no))\n",
    "inp3_final=dataset_final['delivery_time']\n",
    "inp4_final=dataset_final['blood_pressure']\n",
    "inp5_final=dataset_final['heart_problem']\n",
    "\n",
    "frames1_final=[inp1_final,inp2_final,inp3_final,inp4_final,inp5_final]\n",
    "inp_final=pd.concat(frames1_final,axis=1)\n",
    "print(inp_final)\n",
    "\n",
    "out_final = dataset_final['caesarian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pantp\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\pantp\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#making the dimensions same:\n",
    "\n",
    "new_1=np.append(inp_final,0).reshape(1, -1)\n",
    "new_2=np.append(new_1,0).reshape(1, -1)\n",
    "new_3=np.append(new_2,0).reshape(1, -1)\n",
    "new_4=np.append(new_3,0).reshape(1, -1)\n",
    "#print(new_4)\n",
    "                \n",
    "# scaling the  data\n",
    "\n",
    "scaler_final = preprocessing.StandardScaler().fit(new_4)\n",
    "scaled_final = scaler_final.transform(new_4)\n",
    "#print(scaled_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class [1]\n",
      "class_probability [[0.41175073 0.58824927]]\n",
      "class [1]\n",
      "class_probability [[0.40500112 0.59499888]]\n"
     ]
    }
   ],
   "source": [
    "#predicting the output using 1st model:\n",
    "\n",
    "output_1=clf_L2.predict(scaled_final)\n",
    "print('class',output_1)\n",
    "\n",
    "p_output_1=clf_L2.predict_proba(scaled_final)\n",
    "print('class_probability',p_output_1)\n",
    "\n",
    "#predicting the output using 2nd model:\n",
    "output_2=ev_clf_L2.predict(scaled_final)\n",
    "print('class',output_2)\n",
    "\n",
    "p_output_2=ev_clf_L2.predict_proba(scaled_final)\n",
    "print('class_probability',p_output_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
